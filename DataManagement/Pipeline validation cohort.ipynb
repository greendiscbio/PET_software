{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583648c3-70d7-44cb-900d-8343e8ed75d9",
   "metadata": {},
   "source": [
    "Notebook used to parse the raw reports generated with SPM and build the data matrices with hypometabolism data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa79b1e-e297-4190-b3bd-844af68ff638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import hamming\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44292926-776c-4e63-9443-8cd446da08b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_REPORTS = {\n",
    "    'AD'   : os.path.join('..', 'Data', 'DatasetforValidationReport_FDGPET_AD'),\n",
    "    'bvFTD': os.path.join('..', 'Data', 'DatasetforValidationReport_FDGPET_FTD')\n",
    "}\n",
    "COGNITIVE_DATA_FILE = os.path.join('..', 'Data', 'DatasetforValidation.csv')\n",
    "\n",
    "AAL_ROIS = [\n",
    " 'precentral_l', 'precentral_r', 'postcentral_l', 'postcentral_r', 'rolandic_oper_l', 'rolandic_oper_r', \n",
    "    'frontal_sup_l', 'frontal_sup_r', 'frontal_mid_l', 'frontal_mid_r', 'frontal_inf_oper_l', 'frontal_inf_oper_r', \n",
    "    'frontal_inf_tri_l', 'frontal_inf_tri_r', 'frontal_sup_medial_l', 'frontal_sup_medial_r', 'supp_motor_area_l', \n",
    "    'supp_motor_area_r', 'paracentral_lobule_l', 'paracentral_lobule_r', 'frontal_sup_orb_l', 'frontal_sup_orb_r', \n",
    "    'frontal_med_orb_l', 'frontal_med_orb_r', 'frontal_mid_orb_l', 'frontal_mid_orb_r', 'frontal_inf_orb_l', \n",
    "    'frontal_inf_orb_r', 'rectus_l', 'rectus_r', 'olfactory_l', 'olfactory_r', 'temporal_sup_l', 'temporal_sup_r', \n",
    "    'heschl_l', 'heschl_r', 'temporal_mid_l', 'temporal_mid_r', 'temporal_inf_l', 'temporal_inf_r', 'parietal_sup_l', \n",
    "    'parietal_sup_r', 'parietal_inf_l', 'parietal_inf_r', 'angular_l', 'angular_r', 'supramarginal_l', 'supramarginal_r', \n",
    "    'precuneus_l', 'precuneus_r', 'occipital_sup_l', 'occipital_sup_r', 'occipital_mid_l', 'occipital_mid_r', \n",
    "    'occipital_inf_l', 'occipital_inf_r', 'cuneus_l', 'cuneus_r', 'calcarine_l', 'calcarine_r', 'lingual_l', \n",
    "    'lingual_r', 'fusiform_l', 'fusiform_r', 'temporal_pole_sup_l', 'temporal_pole_sup_r', 'temporal_pole_mid_l', \n",
    "    'temporal_pole_mid_r', 'cingulum_ant_l', 'cingulum_ant_r', 'cingulum_mid_l', 'cingulum_mid_r', 'cingulum_post_l', \n",
    "    'cingulum_post_r', 'hippocampus_l', 'hippocampus_r', 'parahippocampal_l', 'parahippocampal_r', 'insula_l', 'insula_r', \n",
    "    'amygdala_l', 'amygdala_r', 'caudate_l', 'caudate_r', 'putamen_l', 'putamen_r', 'pallidum_l', 'pallidum_r', \n",
    "    'thalamus_l', 'thalamus_r'\n",
    "]\n",
    "\n",
    "EXCLUDE_ROIS = [\n",
    "    'cerebelum_4_5_r', 'cerebelum_3_r', 'cerebelum_6_r', 'cerebelum_6_r', 'cerebelum_crus2_r', 'cerebelum_crus2_l',\n",
    "    'cerebelum_crus1_l', 'cerebelum_crus1_r', 'cerebelum_10_l', 'cerebelum_4_5_l', 'cerebelum_9_l', 'cerebelum_8_r', \n",
    "    'cerebelum_7b_l', 'cerebelum_10_r', 'cerebelum_6_l', 'cerebelum_8_l', 'cerebelum_7b_r', 'vermis_6',\n",
    "    'cerebelum_3_l'\n",
    "]\n",
    "\n",
    "# Dictionary with the cognitive variables used and nomenclature in English\n",
    "COGNITIVE_TEST = {\n",
    "    # digit span forward and backward\n",
    "    \"Spandirecto\": \"Digit span forward\",\n",
    "    \"Spanindirecto\": \"Digit span backward\",\n",
    "    \n",
    "    # Corsi’s test forward and backward,\n",
    "    \"Corsidirecto\": \"Corsi’s test forward\",                 \n",
    "    \"Corsiinverso\": \"Corsi’s test backward\",    \n",
    "    \n",
    "    # Trail Making Test part A and B\n",
    "    \"TMTa\": \"TMT A\",\n",
    "    \"TMTb\": \"TMT B\",\n",
    "    \n",
    "    # Symbol Digit Modalities Test\n",
    "    \"SDMT\": \"SDMT\",\n",
    "    \n",
    "    # Stroop Color-Word Interference test\n",
    "    \"Stroop1\": \"SCWIT word reading\",\n",
    "    \"Stroop2\": \"SCWIT color naming\",\n",
    "    \"Stroop3\": \"SCWIT interference\",\n",
    "    \n",
    "    # Tower of London-Drexel version\n",
    "    \"ToLtotalcorrectos\": \"ToL correct\",\n",
    "    \"ToLmovtotales\"    : \"ToL movements\",\n",
    "    \"ToLinicio\"        : \"ToL start\",\n",
    "    \"ToLejecucion\"     : \"ToL execution\",\n",
    "    \"ToLresolucion\"    : \"ToL resolution\",\n",
    "    \n",
    "    # Boston Naming Test\n",
    "    \"BostonNT\": \"BNT\",\n",
    "    \n",
    "    # Semantic fluency (animals)\n",
    "    \"Fluenciaanimalesescalar\": \"Semantic fluency\", \n",
    "    \n",
    "    # Letter fluency (words beginning with “p”),\n",
    "    \"FluenciaPescalar\": \"Letter fluency\",\n",
    "    \n",
    "    # Free and Cued Selective Reminding Test\n",
    "    \"FCSRTL1\"      : \"FCSRT free recall 1\",                           \n",
    "    \"FCSRTLT\"      : \"FCSRT total free recall\",                           \n",
    "    \"FCSRTtotal\"   : \"FCSRT total recall\",                     \n",
    "    \"FCSRTdifLibre\": \"FCSRT delayed free recall\",               \n",
    "    \"FCSRTdifTOTAL\": \"FCSRT delayed total recall\",       \n",
    "    \n",
    "    # Rey-Osterrieth Complex Figure (copy accuracy, copy time, memory at 3 and 30 minutes, and recognition memory)\n",
    "    \"ReyCopia\"         : \"ROCF copy accuracy\",\n",
    "    \"Rey3min\"          : \"ROCF memory 3 min\",\n",
    "    \"Rey30min\"         : \"ROCF memory 30 min\" ,\n",
    "    \"ReyTiempo\"        : \"ROCF time\",\n",
    "    \"ReyReconocimiento\": \"ROCF recognition\",\n",
    "    \n",
    "    # Benton’s Judgment Line Orientation,\n",
    "    \"JLOescalar\": \"JLO\",\n",
    "    \n",
    "    # Visual Object and Space Perception Battery (object decision, progressive silhouettes, \n",
    "    # discrimination of position, and number location\n",
    "    \"VOSPdecision\"     : \"VOSP object decision\",\n",
    "    \"VOSPsiluetas\"     : \"VOSP progressive silhouettes\",\n",
    "    \"VOSPdiscriposi\"   : \"VOSP discrimination of position\",\n",
    "    \"VOSPlocaliznumero\": \"VOSP number location\",\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Dictionary with the demographic variables\n",
    "DEMO_DATA = {\n",
    "    'Sexo': 'sex',\n",
    "    'Edadactual': 'age',\n",
    "    'AñosEducacion': 'years_of_formal_education'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a335b5b-7a35-427f-9c8e-aef2960cde0c",
   "metadata": {},
   "source": [
    "# Cognitive test processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062bfb3e-6798-4942-81e1-1c120a8ec776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and process cognitive data (same pipeline as with the training data)\n",
    "cognitive_data = pd.read_csv(COGNITIVE_DATA_FILE, sep=';') \n",
    "\n",
    "unique_pet_ids = cognitive_data['CD_PET'].astype(int).values.tolist()\n",
    "\n",
    "assert len(set(unique_pet_ids)) == len(unique_pet_ids), 'data contains duplicated'\n",
    "\n",
    "cognitive_data = cognitive_data.set_index('CD_PET')[    \n",
    "    list(DEMO_DATA.keys()) +\n",
    "    list(COGNITIVE_TEST.keys())\n",
    "].rename(columns={\n",
    "    **DEMO_DATA,\n",
    "    **COGNITIVE_TEST,\n",
    "}, errors='raise').copy()\n",
    "\n",
    "# convert tests to float\n",
    "for cog_test in COGNITIVE_TEST.values():\n",
    "    cognitive_data[cog_test] = cognitive_data[cog_test].apply(lambda v: float(v) if v != ' ' else np.nan)\n",
    "\n",
    "# convert demographic variables to float\n",
    "cognitive_data['age'] = cognitive_data['age'].astype(float)\n",
    "cognitive_data['years_of_formal_education'] = cognitive_data['years_of_formal_education'].astype(float)\n",
    "\n",
    "cognitive_data.loc[cognitive_data[\"SCWIT color naming\"] > 20, 'SCWIT color naming'] = np.nan\n",
    "cognitive_data.loc[cognitive_data[\"SCWIT interference\"] < 2.0, \"SCWIT interference\"] = np.nan\n",
    "cognitive_data.loc[cognitive_data[\"BNT\"] > 20, 'BNT'] = np.nan\n",
    "\n",
    "# for patients with dementia imputate the missing values in ToL and Rey\n",
    "# using the lower values as these patients didn't complete the test because\n",
    "# they cognitive impairment\n",
    "variables_floor_imputation = [\n",
    "    \"ToL correct\",\n",
    "    \"ToL movements\",\n",
    "    \"ToL start\",\n",
    "    \"ToL execution\",\n",
    "    \"ToL resolution\",\n",
    "    \"ROCF copy accuracy\",\n",
    "    \"ROCF memory 3 min\",\n",
    "    \"ROCF memory 30 min\" ,\n",
    "    \"ROCF time\",\n",
    "    \"ROCF recognition\",\n",
    "]\n",
    "\n",
    "for var in variables_floor_imputation:\n",
    "    cognitive_data.loc[cognitive_data[var].isna(), var] = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c8af6-b013-41ff-acff-1bb6e75c12f1",
   "metadata": {},
   "source": [
    "# Add FDG-PET information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11085027-33da-4e0f-8dfa-c377c956d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = []\n",
    "verbose = False\n",
    "for key, file in INPUT_REPORTS.items():\n",
    "    # read file content\n",
    "    with open(file) as f:\n",
    "        file_content = f.read()\n",
    "    \n",
    "    # extract information\n",
    "    pet_ids = {}\n",
    "    valid_line = False\n",
    "    for line in file_content.split('\\n'):\n",
    "\n",
    "        # search for PET ids\n",
    "        pet_id_match = re.search(r'/(\\d+)/', line)\n",
    "\n",
    "        # line associated with a PET id\n",
    "        if pet_id_match:\n",
    "            cd_pet = int(pet_id_match.group(1))\n",
    "\n",
    "            # check if the extracted id is the cognitive data ids\n",
    "            if cd_pet not in unique_pet_ids:\n",
    "                if verbose:\n",
    "                    print('ERROR. CD_PET \"{}\" not found'.format(cd_pet))\n",
    "                valid_line = False\n",
    "            else:\n",
    "                pet_ids[cd_pet] = {\n",
    "                    roi: 0 for roi in AAL_ROIS\n",
    "                }\n",
    "                valid_line = True\n",
    "                if verbose:\n",
    "                    print('Success match for CD_PET \"{}\"'.format(cd_pet))\n",
    "\n",
    "        else:\n",
    "            if not valid_line:   # missing cd_pet\n",
    "                continue\n",
    "\n",
    "            # search for number of hypometabolic voxels\n",
    "            roi_hypo_match = re.match(r'^\\d.*', line)\n",
    "\n",
    "            # there have been a match\n",
    "            if roi_hypo_match:\n",
    "                roi_match = None\n",
    "\n",
    "                # process AAL ROIs\n",
    "                if '(aal)' in line:\n",
    "                    line = line.replace('(aal)', '')\n",
    "                    roi_match = re.match(r'(\\d+)\\s+(.*?)\\s+', line)\n",
    "\n",
    "                ## process Brodman ROIs (omitted)\n",
    "                #if 'brodmann' in line:\n",
    "                #    line = line.replace(' area ', '_')\n",
    "                #    roi_match = re.match(r'(\\d+)\\s+(.*)', line)\n",
    "\n",
    "                if roi_match:\n",
    "\n",
    "                    # extract ROI label and number of hypometabolic voxels\n",
    "                    nvoxels, roi = int(roi_match.group(1)), roi_match.group(2).lower()\n",
    "\n",
    "                    # omit ROI\n",
    "                    if roi in EXCLUDE_ROIS:\n",
    "                        continue\n",
    "\n",
    "                    # save ROI information   \n",
    "                    pet_ids[cd_pet][roi] = nvoxels\n",
    "\n",
    "    pet_df = pd.DataFrame(pet_ids).T\n",
    "    pet_df['diagnosis'] = key\n",
    "    final_df.append(pet_df)\n",
    "\n",
    "final_df = pd.concat(final_df, axis=0)\n",
    "final_df.index.names = ['CD_PET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96834e6-824f-40a1-b0c0-b533552869b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge cognitive data and FDG-PET data \n",
    "df = cognitive_data.join(final_df, how='inner')\n",
    "assert not df.index.duplicated().any()\n",
    "\n",
    "# add diagnosis information to match train data structure \n",
    "df['diagnosis_AD'] = 0\n",
    "df['diagnosis_HC'] = 0\n",
    "df['diagnosis_bvFTD'] = 0\n",
    "\n",
    "df.loc[df.diagnosis == 'bvFTD', 'diagnosis_bvFTD'] = 1\n",
    "df.loc[df.diagnosis == 'AD', 'diagnosis_AD'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13554196-9cf1-4326-bda4-a9a181649190",
   "metadata": {},
   "source": [
    "# Analyze missing values profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ce35c3-1a7a-4308-b914-01793a201fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_missing</th>\n",
       "      <th>perc_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JLO</th>\n",
       "      <td>8</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCSRT total free recall</th>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCSRT delayed total recall</th>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCSRT delayed free recall</th>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCSRT total recall</th>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BNT</th>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCSRT free recall 1</th>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TMT B</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOSP number location</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCWIT interference</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCWIT color naming</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCWIT word reading</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDMT</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOSP object decision</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOSP progressive silhouettes</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOSP discrimination of position</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TMT A</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 n_missing  perc_missing\n",
       "JLO                                      8          20.0\n",
       "FCSRT total free recall                  3           7.5\n",
       "FCSRT delayed total recall               3           7.5\n",
       "FCSRT delayed free recall                3           7.5\n",
       "FCSRT total recall                       3           7.5\n",
       "BNT                                      3           7.5\n",
       "FCSRT free recall 1                      3           7.5\n",
       "TMT B                                    2           5.0\n",
       "VOSP number location                     2           5.0\n",
       "SCWIT interference                       1           2.5\n",
       "SCWIT color naming                       1           2.5\n",
       "SCWIT word reading                       1           2.5\n",
       "SDMT                                     1           2.5\n",
       "VOSP object decision                     1           2.5\n",
       "VOSP progressive silhouettes             1           2.5\n",
       "VOSP discrimination of position          1           2.5\n",
       "TMT A                                    1           2.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_profile = pd.concat([\n",
    "    df.isna().sum(),\n",
    "    (df.isna().sum() / df.shape[0]) * 100\n",
    "], axis=1)\n",
    "missing_profile.columns = ['n_missing', 'perc_missing']\n",
    "missing_profile = missing_profile.loc[missing_profile['n_missing'] > 0].copy().sort_values(by='n_missing', ascending=False)\n",
    "missing_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995034a1-ea8a-40e2-a4fe-3af366cdcc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subject_miss_perc = pd.DataFrame(\n",
    "    df[list(COGNITIVE_TEST.values())].isna().sum(axis=1) / len(COGNITIVE_TEST) * 100, \n",
    "    columns=['perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0fd721-8c95-47ee-b51b-14523629bcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform imputation based on training data\n",
    "train_data = pd.read_parquet(os.path.join('..', 'data', 'final_data_MICE.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c015f35-966e-46e6-9c62-10a2c2e06ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a copy of the data to be imputed\n",
    "train_data_mice = train_data[\n",
    "    list(COGNITIVE_TEST.values()) + \n",
    "    list(DEMO_DATA.values())].copy()\n",
    "test_data_mice = df[\n",
    "    list(COGNITIVE_TEST.values()) + \n",
    "    list(DEMO_DATA.values())].copy()\n",
    "\n",
    "# create imputer (same configuration than the one used by Pedro)\n",
    "mice_imputer = IterativeImputer(\n",
    "    estimator=BayesianRidge(), \n",
    "    max_iter=100,\n",
    "    random_state=1997\n",
    ")\n",
    "\n",
    "# standarize the data\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train_data_mice)\n",
    "train_data_mice_std = scaler.transform(train_data_mice)\n",
    "test_data_mice_std = scaler.transform(test_data_mice)\n",
    "\n",
    "# fit estimator\n",
    "mice_imputer_fitted = mice_imputer.fit(train_data_mice_std)\n",
    "\n",
    "test_data_mice_imp = mice_imputer_fitted.transform(test_data_mice_std)\n",
    "\n",
    "# de-standarize the variables\n",
    "test_data_mice_imp = scaler.inverse_transform(test_data_mice_imp)\n",
    "test_data_mice_imp = pd.DataFrame(\n",
    "    test_data_mice_imp, \n",
    "    columns=test_data_mice.columns,\n",
    "    index=test_data_mice.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4a3ad-b7e0-4a87-abc9-bc2d75bfc070",
   "metadata": {},
   "source": [
    "# Data exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "550ec424-b9ba-425b-ba48-9a961bcfcbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format the final dataframe\n",
    "df_test_final_imp = test_data_mice_imp.join(\n",
    "    df[[c for c in df.columns if c not in test_data_mice_imp.columns]],\n",
    "    how='inner')\n",
    "\n",
    "# export the data\n",
    "# df_test_final_imp_subsample.to_parquet(os.path.join('..', 'data', 'final_data_MICE_val_cohort_v1.parquet'))\n",
    "# df_test_final_imp_subsample.to_csv(os.path.join('..', 'data', 'final_data_MICE_val_cohort_v1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64eb4cf-2a31-45c6-9ead-cc94a49b1062",
   "metadata": {},
   "source": [
    "# Compare brain hypometabolism between cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f741b3-075a-4b77-b8f1-85e0e89b6b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for condition in ['bvFTD', 'AD']:\n",
    "    cognition_comparison = []\n",
    "    metabolism_comparison = []\n",
    "\n",
    "    train_data_cond = train_data.loc[train_data.diagnosis == condition]\n",
    "    test_data_cond = df_test_final_imp.loc[df_test_final_imp.diagnosis == condition]\n",
    "    \n",
    "    # compare cognitive tests\n",
    "    for cog_test in COGNITIVE_TEST.values():\n",
    "        cog_test_stat = pingouin.ttest(\n",
    "            train_data_cond[cog_test].values,\n",
    "            test_data_cond[cog_test].values\n",
    "        )\n",
    "        cog_test_stat['diagnosis'] = condition\n",
    "        cog_test_stat['mean_train'] = train_data_cond[cog_test].mean()\n",
    "        cog_test_stat['mean_valid'] = test_data_cond[cog_test].mean()\n",
    "        cog_test_stat['std_train'] = train_data_cond[cog_test].std()\n",
    "        cog_test_stat['std_valid'] = test_data_cond[cog_test].std()\n",
    "        cog_test_stat.index = [cog_test]\n",
    "        cog_test_stat.index.names = ['Test']\n",
    "        cognition_comparison.append(cog_test_stat)\n",
    "        \n",
    "    for roi in AAL_ROIS:\n",
    "        # calculate proportions\n",
    "        train_data_prop30 = np.sum((train_data_cond[roi] >= 30)) / len(train_data_cond) * 100\n",
    "        test_data_prop30 = np.sum((test_data_cond[roi] >= 30)) / len(test_data_cond) * 100\n",
    "        train_data_prop100 = np.sum((train_data_cond[roi] >= 100)) / len(train_data_cond) * 100\n",
    "        test_data_prop100 = np.sum((test_data_cond[roi] >= 100)) / len(test_data_cond) * 100\n",
    "        \n",
    "        if train_data_prop30 < 5 or train_data_prop100 < 5:\n",
    "            continue\n",
    "\n",
    "        # create contingency table\n",
    "        contingency_table30 = np.array([\n",
    "            [np.sum((train_data_cond[roi] < 30)), np.sum((train_data_cond[roi] >= 30))],\n",
    "            [np.sum((test_data_cond[roi] < 30)), np.sum((test_data_cond[roi] >= 30))]])\n",
    "\n",
    "        # perform chi2\n",
    "        chi230, p_val30, _, _ = chi2_contingency(contingency_table30)\n",
    "\n",
    "        # create contingency table\n",
    "        contingency_table100 = np.array([\n",
    "            [np.sum((train_data_cond[roi] < 100)), np.sum((train_data_cond[roi] >= 100))],\n",
    "            [np.sum((test_data_cond[roi] < 100)), np.sum((test_data_cond[roi] >= 100))]])\n",
    "\n",
    "        # perform chi2\n",
    "        chi2100, p_val100, _, _ = chi2_contingency(contingency_table100)\n",
    "\n",
    "            \n",
    "        metabolism_comparison.append({\n",
    "            'ROI': roi,\n",
    "\n",
    "            'prop_train (th 30)': train_data_prop30,\n",
    "            'prop_valid (th 30)': test_data_prop30,\n",
    "            'diff propotion (th 30)': train_data_prop30 - test_data_prop30,\n",
    "            'chi2 statistic (th 30)': chi230,\n",
    "            'p-value (th 30)': p_val30,\n",
    "\n",
    "            'prop_train (th 100)': train_data_prop100,\n",
    "            'prop_valid (th 100)': test_data_prop100,\n",
    "            'diff propotion (th 100)': train_data_prop100 - test_data_prop100,\n",
    "            'chi2 statistic (th 100)': chi2100,\n",
    "            'p-value (th 100)': p_val100,\n",
    "        })\n",
    "        \n",
    "    # format dataframes\n",
    "    cognition_comparison_df = pd.concat(cognition_comparison, axis=0)\n",
    "    cognition_comparison_df = cognition_comparison_df.drop(columns=['dof', 'alternative', 'BF10'])\n",
    "\n",
    "    metabolism_comparison_df = pd.DataFrame(metabolism_comparison)\n",
    "    metabolism_comparison_df = metabolism_comparison_df.set_index('ROI')\n",
    "    \n",
    "    # export dataframes\n",
    "    cognition_comparison_df.to_csv(\n",
    "        os.path.join(\n",
    "            '..', 'results', '%s_cognition_ttest_%s_v1.csv' % (datetime.now().strftime('%Y%m%d'), condition) \n",
    "        ))\n",
    "    metabolism_comparison_df.to_csv(\n",
    "        os.path.join(\n",
    "            '..', 'results', '%s_metabolism_chi2_%s_v1.csv' % (datetime.now().strftime('%Y%m%d'), condition) \n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dff7c-183f-42f6-9706-f38e12c80e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c5793-f834-46a1-a599-854358c61e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9bedf-63d1-4a6a-9b1b-1cdf926c0bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74e876-11bd-468a-bd8a-cca1c356b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cabdb4-06ac-4bf8-8bb3-fe17491886c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7e852-86b0-4b4a-a28e-3d360d704848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723039d-ab15-4f00-918d-571eebfa82d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516157ca-d6f4-40c8-8a39-18f80aed0217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eed2ff-135b-41a8-aba6-e592dc4ed021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fba363-6738-4087-ab16-078bf7d2e80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b7ebe-ca45-40b9-b985-83436e43f8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b0a59-5c10-4676-8791-336bf4f966f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a3337-55dd-4dbc-8753-89d0456e7447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f3dc7-4378-48be-bc54-c858424875fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72539d-a5c6-40e2-b4f0-3013ac24b2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fc05f-b381-442a-81d9-cef42e0204f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d80c03-d192-403a-8b8f-a5bb9ba2b7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlv0",
   "language": "python",
   "name": "mlv0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
